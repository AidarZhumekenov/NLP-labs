{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "96cRA7pZ-x44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ffb918d0-6323-4f32-de7b-b0e34b92ac0e"
      },
      "source": [
        "import nltk;\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "import re, string, random\n",
        "import gzip\n",
        "import gensim \n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "        #token = re.sub(\"\", \"\",token)\n",
        "        #token = re.sub(\"\", \"\",token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    stop_words_ru = stopwords.words('russian')\n",
        "    stop_words_ru.extend(['что', 'но','это', 'так', 'вот', 'быть', 'как', 'в', '—', '–', 'к', 'на', '...'])\n",
        "\n",
        "    #Read and Split file\n",
        "    text_lab4 = open('/content/Lab4_text.txt', 'r').read()\n",
        "    text_lab4_splitted = text_lab4.split('\\n')\n",
        "\n",
        "    #lab4_cleaned_tokens_list = []\n",
        "    text_lab4_splitted_tokens = []\n",
        "    text_lab4_splitted_tokens_cleared = []\n",
        "    text_lab4_splitted_tokens_cleared_final = []\n",
        "    \n",
        "    #Tokenize each line by word\n",
        "    for tokens in text_lab4_splitted:\n",
        "      text_lab4_splitted_tokens.append(word_tokenize(tokens))\n",
        "    \n",
        "    \n",
        "    # Remove stop words and punctuation\n",
        "    for tokens in text_lab4_splitted_tokens:\n",
        "      for tokens2 in tokens:\n",
        "        if (tokens2 not in stop_words_ru and tokens2 not in string.punctuation):\n",
        "          text_lab4_splitted_tokens_cleared.append(tokens2)\n",
        "      text_lab4_splitted_tokens_cleared_final.append(\" \".join(text_lab4_splitted_tokens_cleared))\n",
        "      text_lab4_splitted_tokens_cleared=[]\n",
        "\n",
        "    #Prepare 2 sets to merge\n",
        "    ds_sentence = pd.DataFrame(text_lab4_splitted_tokens_cleared_final, columns=['Sentence']) \n",
        "    ds_score = pd.read_csv('/content/scores_train.txt')\n",
        "\n",
        "    #Concatenate Cleaned Sentences with Scores\n",
        "    result = pd.concat([ds_sentence, ds_score], axis=1, sort=False)\n",
        "\n",
        "    #Clean NaN from set    \n",
        "    result = result.dropna()\n",
        "\n",
        "    # Create our vectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    # Get the training vectors\n",
        "    vectors = vectorizer.fit_transform(result['Sentence'])\n",
        "\n",
        "    # Build the classifier\n",
        "    clf = MultinomialNB(alpha=.01)\n",
        "\n",
        "    #  Train the classifier\n",
        "    clf.fit(vectors, result['Sentimental'])\n",
        "\n",
        "    # Get the test vectors\n",
        "    vectors_test = vectorizer.transform(result['Sentence'])\n",
        "\n",
        "    # Predict and score the vectors\n",
        "    pred = clf.predict(vectors_test)\n",
        "    acc_score = metrics.accuracy_score(result['Sentimental'], pred)\n",
        "    f1_score = metrics.f1_score(result['Sentimental'], pred, average='macro')\n",
        "\n",
        "    #Show accuracy:\n",
        "    print('Total accuracy classification score: {}'.format(acc_score))\n",
        "    print('Total F1 classification score: {}'.format(f1_score))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Total accuracy classification score: 0.8488\n",
            "Total F1 classification score: 0.8658512846531389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpg2Py7BmLY",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}